# List of Datasets

| Name | Description | License | Reference |
|------|-------------|----------|---------|
| advglue <br> *advglue-all.json* | Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models.  | CC-BY-4.0 license | https://github.com/AI-secure/adversarial-glue |
| Analogical Similarity <br> *analogical-similarity.json* | To measure the modelâ€™s ability in discriminating between different degrees of analogical similarity in two given episodes | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity |
| Answercarefully Information Cantonese <br> *answercarefully-ca.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Answercarefully Information Chinese <br> *answercarefully-cn.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| Answercarefully Information English <br> *answercarefully-en.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| Answercarefully Information Farsi <br> *answercarefully-fa.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Answercarefully Information French <br> *answercarefully-fr.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Answercarefully Information Japanese <br> *answercarefully-jp.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| Answercarefully Information Korean <br> *answercarefully-kr.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing | Dataset from NII-LLMC working group - subset created for AISI testing |
| Answercarefully Information Swahili <br> *answercarefully-kw.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Answercarefully Information Malay <br> *answercarefully-my.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| Answercarefully Information Telugu <br> *answercarefully-tl.json* | A dataset of security-related questions and answers. | Dataset from NII-LLMC working group - subset created for AISI testing. | Dataset from NII-LLMC working group - subset created for AISI testing. |
| ARC-Challenge <br> *arc-challenge.json* | A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the challenge set. | CC BY-SA | https://allenai.org/data/arc |
| ARC-Easy <br> *arc-easy.json* | A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the easy set. | CC BY-SA | https://allenai.org/data/arc |
| auto_categorization <br> *auto-categorisation.json* | To measure the model's ability in categorising elements into groups. | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization |
| BBQ-lite on age - Ambiguous Questions <br> *bbq-lite-age-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on age, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on age - Disambiguated Questions <br> *bbq-lite-age-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on age, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on disability-status - Ambiguous Questions <br> *bbq-lite-disability-status-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on disability-status - Disambiguated Questions <br> *bbq-lite-disability-status-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on gender - Ambiguous Questions <br> *bbq-lite-gender-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on gender - Disambiguated Questions <br> *bbq-lite-gender-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on nationality - Ambiguous Questions <br> *bbq-lite-nationality-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on nationality - Disambiguated Questions <br> *bbq-lite-nationality-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on physical-appearance - Ambiguous Questions <br> *bbq-lite-physical-appearance-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on physical-appearance - Disambiguated Questions <br> *bbq-lite-physical-appearance-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-ethnicity - Ambiguous Questions <br> *bbq-lite-race-ethnicity-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-ethnicity - Disambiguated Questions <br> *bbq-lite-race-ethnicity-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-x-SES - Ambiguous Questions <br> *bbq-lite-race-x-ses-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on race-x-SES, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-x-SES - Disambiguated Questions <br> *bbq-lite-race-x-ses-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on race-x-SES, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-x-gender - Ambiguous Questions <br> *bbq-lite-race-x-gender-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on race-x-gender - Disambiguated Questions <br> *bbq-lite-race-x-gender-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on religion - Ambiguous Questions <br> *bbq-lite-religion-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on religion - Disambiguated Questions <br> *bbq-lite-religion-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on SES - Ambiguous Questions <br> *bbq-lite-ses-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on SES, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on SES - Disambiguated Questions <br> *bbq-lite-ses-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on SES, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on sexual-orientation - Ambiguous Questions <br> *bbq-lite-sexual-orientation-ambiguous.json* | This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the ambiguous questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| BBQ-lite on sexual-orientation - Disambiguated Questions <br> *bbq-lite-sexual-orientation-disamb.json* | This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the disambiguated questions. | CC-BY-4.0 license | https://arxiv.org/pdf/2110.08193v2 |
| cause_and_effect <br> *cause-and-effect.json* | This recipe measures if the model is able to distinguish and understand cause and effect. The higher the grade, the better the system can perform this capability. | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-age-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-age-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-disability-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-disability-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-disease-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-disease-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-educational-qualification-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-educational-qualification-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-ethnicity-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-ethnicity-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-gender-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-gender-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-household-registration-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-household-registration-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-nationality-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-nationality-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-physical-appearance-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-physical-appearance-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-race-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-race-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-region-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-region-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-religion-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-religion-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-SES-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-SES-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-sexual-orientation-ambiguous.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| Chinese Version - Bias Benchmark for QA <br> *cbbq-lite-sexual-orientation-disamb.json* | This aims to measure social biases across 9 different categories in Chinese language. | CC-BY-SA-4.0 license | https://github.com/YFHuangxxxx/CBBQ |
| ChallengingToxicityPrompts <br> *challenging-toxicity-prompts-variation1.json* | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| ChallengingToxicityPrompts <br> *challenging-toxicity-prompts-variation2.json* | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| ChallengingToxicityPrompts <br> *challenging-toxicity-prompts.json* | ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences. | CC BY-SA 4.0 | https://github.com/AI-secure/DecodingTrust |
| Chinese Linguistics & Cognition Challenge (CLCC) <br> *clcc.json* | This dataset is a subjective benchmark created by BAAI FlagEval group. | CC-BY-4.0 license | https://flageval.baai.ac.cn/ |
| Ethics-Commonsense-Easy <br> *commonsense-morality-easy-variation1.json* | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Ethics-Commonsense-Easy <br> *commonsense-morality-easy-variation2.json* | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Ethics-Commonsense-Hard <br> *commonsense-morality-hard-variation1.json* | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Ethics-Commonsense-Hard <br> *commonsense-morality-hard-variation2.json* | This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept. | MIT License | https://github.com/hendrycks/ethics/ |
| Contextual Parametric Knowledge Conflicts <br> *contextual-parametric-knowledge-conflicts.json* | To measure the model's ability in making informed predictions | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/contextual_parametric_knowledge_conflicts |
| coqa_conversational_question_answering <br> *coqa-conversational-qna.json* | To measure the ability of machines to understand a text passage and answer a series of interconnected questions | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/coqa_conversational_question_answering |
| C-Values - Values of Chinese Large Language attack_modules <br> *cvalues.json* | This dataset contains prompts that can measure the alignment ability of LLMs in both safety and responsibility in Chinese Language. | Apache-2.0 license | https://github.com/X-PLUG/CValues |
| CyberSecEval Prompt Injection 2 Chinese <br> *cyberseceval-promptinjection2-cn.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 English <br> *cyberseceval-promptinjection2-en.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 French <br> *cyberseceval-promptinjection2-fr.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 Japanese <br> *cyberseceval-promptinjection2-jp.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 Korean <br> *cyberseceval-promptinjection2-kr.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 Swahili <br> *cyberseceval-promptinjection2-kw.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection 2 Malay <br> *cyberseceval-promptinjection2-my.json* | A dataset of security-related questions and answers. | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| CyberSecEval Prompt Injection <br> *cyberseceval_promptinjection.json* | Check if llm is susceptible to prompt injection | MIT license | https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection |
| Facts about Asia pacific in True and False in 4 languages (Chinese, Malay, Tamil and English) <br> *facticity-apac-multilungual-facts.json* | Contain prompts that contain facts about Asia Pacific and Singapore, in True/False format | Apache-2.0 | facticity.ai |
| uciadult <br> *fairness-uciadult.json* | The UCI Adult dataset has been used widely used to assess fairness. | Creative Commons Attribution 4.0 International | https://archive.ics.uci.edu/dataset/2/adult |
| Gender Occupational Bias <br> *gender-occupation-text2-image-prompts.json* | The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759 | MIT License | https://arxiv.org/abs/2211.03759 |
| Gender Occupational Bias <br> *gender-text2-image-prompts.json* | The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759 | MIT License | https://arxiv.org/abs/2211.03759 |
| gre_reading_comprehension <br> *gre-reading-comprehension.json* | To measure the model/'s ability to summarize text, infer and deduce knowledge from context, and match the context. | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension |
| GSM8K-TEST <br> *gsm8k-test.json* | This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. The dataset includes only the test set of GSM8K. | MIT License | https://github.com/openai/grade-school-math |
| GSM8K <br> *gsm8k.json* | This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. | MIT License | https://github.com/openai/grade-school-math |
| HellaSwag <br> *hellaswag.json* | This dataset is used to evaluate commonsense with questions that are trivial for humans but difficult for state-of-the-art models. | MIT License | https://github.com/rowanz/hellaswag |
| Inappropriate Image Prompts (I2P) <br> *i2p-text2image-prompts.json* | The I2P benchmark contains real user prompts for generative text2image prompts that are unproportionately likely to produce inappropriate images. I2P was introduced in the 2023 CVPR paper Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. This benchmark is not specific to any approach or model, but was designed to evaluate mitigating measures against inappropriate degeneration in Stable Diffusion. | MIT License | https://huggingface.co/datasets/AIML-TUDA/i2p |
| In-the-wild Jailbreak Dataset <br> *jailbreak-dan.json* | This dataset contains jailbreak prompts that were collected from various places (e.g., Discord). | MIT license | https://github.com/verazuo/jailbreak_llms |
| Joint Testing 3 - All - Kenya AISI - Chinese - English tool names <br> *jt3-all-kenyaAISI-ch-en-toolnames.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Chinese <br> *jt3-all-kenyaAISI-ch.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Chinese by the Australia AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - English <br> *jt3-all-kenyaAISI-en.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, in English. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Farsi <br> *jt3-all-kenyaAISI-fa.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Farsi by the Canadian AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - French <br> *jt3-all-kenyaAISI-fr.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into French by the France AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Hindi <br> *jt3-all-kenyaAISI-hn.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Hindi by the Singapore AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Japanese - English tool names <br> *jt3-all-kenyaAISI-ja-en-toolnames.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Japanese <br> *jt3-all-kenyaAISI-ja.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Japanese by the Japan AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Korean - English tool names <br> *jt3-all-kenyaAISI-kr-en-toolnames.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Korean <br> *jt3-all-kenyaAISI-kr.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Korean by the Korea AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Swahili <br> *jt3-all-kenyaAISI-kw.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Swahili by the Kenya AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - All - Kenya AISI - Telugu <br> *jt3-all-kenyaAISI-tl.json* | Agentic testing dataset from Kenya AISI, concerning data leakage and fraud risks, translated into Telugu by the Canada AISI. | MIT License | Kenya AISI |
| Joint Testing 3 - Fraud - AgentHarm - Chinese - English tool names <br> *jt3-fraud-ah-ch-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Chinese <br> *jt3-fraud-ah-ch.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Chinese by the Australia AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - English <br> *jt3-fraud-ah-en.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, in English. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Farsi <br> *jt3-fraud-ah-fa.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Farsi by the Canadian AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - French <br> *jt3-fraud-ah-fr.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into French by the France AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Hindi <br> *jt3-fraud-ah-hn.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Hindi by the Singapore AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Japanese - English tool names <br> *jt3-fraud-ah-ja-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Japanese <br> *jt3-fraud-ah-ja.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Japanese by the Japan AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Korean - English tool names <br> *jt3-fraud-ah-kr-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Korean <br> *jt3-fraud-ah-kr.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Korean by the Korea AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Swahili <br> *jt3-fraud-ah-kw.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Swahili by the Kenya AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - AgentHarm - Telugu <br> *jt3-fraud-ah-tl.json* | Agentic testing dataset adapted from AgentHarm, concerning fraud risks, translated into Telugu by the Canada AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Chinese - English tool names <br> *jt3-fraud-asb-ch-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Chinese <br> *jt3-fraud-asb-ch.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Chinese by the Australia AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - English <br> *jt3-fraud-asb-en.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, in English. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Farsi <br> *jt3-fraud-asb-fa.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Farsi by the Canadian AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - French <br> *jt3-fraud-asb-fr.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into French by the France AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Hindi <br> *jt3-fraud-asb-hn.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Hindi by the Singapore AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Japanese - English tool names <br> *jt3-fraud-asb-ja-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Japanese <br> *jt3-fraud-asb-ja.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Japanese by the Japan AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Korean - English tool names <br> *jt3-fraud-asb-kr-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Korean <br> *jt3-fraud-asb-kr.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Korean by the Korea AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Swahili <br> *jt3-fraud-asb-kw.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Swahili by the Kenya AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - Agent-SafetyBench - Telugu <br> *jt3-fraud-asb-tl.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning fraud risks, translated into Telugu by the Canada AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Fraud - BrowserART - Chinese - English tool names <br> *jt3-fraud-browser-ch-en-toolnames.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Chinese <br> *jt3-fraud-browser-ch.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Chinese by the Australia AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - English <br> *jt3-fraud-browser-en.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, in English. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Farsi <br> *jt3-fraud-browser-fa.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Farsi by the Canadian AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - French <br> *jt3-fraud-browser-fr.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into French by the France AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Hindi <br> *jt3-fraud-browser-hn.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Hindi by the Singapore AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Japanese - English tool names <br> *jt3-fraud-browser-ja-en-toolnames.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Japanese <br> *jt3-fraud-browser-ja.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Japanese by the Japan AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Korean - English tool names <br> *jt3-fraud-browser-kr-en-toolnames.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Korean <br> *jt3-fraud-browser-kr.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Korean by the Korea AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Swahili <br> *jt3-fraud-browser-kw.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Swahili by the Kenya AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - BrowserART - Telugu <br> *jt3-fraud-browser-tl.json* | Agentic testing dataset adapted from BrowserART, concerning fraud risks, translated into Telugu by the Canada AISI. | MIT License | https://github.com/scaleapi/browser-art |
| Joint Testing 3 - Fraud - France AISI - Chinese - English tool names <br> *jt3-fraud-franceAISI-ch-en-toolnames.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Chinese <br> *jt3-fraud-franceAISI-ch.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Chinese by the Australia AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - English <br> *jt3-fraud-franceAISI-en.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, in English | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Farsi <br> *jt3-fraud-franceAISI-fa.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Farsi by the Canadian AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - French <br> *jt3-fraud-franceAISI-fr.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into French by the France AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Hindi <br> *jt3-fraud-franceAISI-hn.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Hindi by the Singapore AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Japanese - English tool names <br> *jt3-fraud-franceAISI-ja-en-toolnames.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Japanese <br> *jt3-fraud-franceAISI-ja.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Japanese by the Japan AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Korean - English tool names <br> *jt3-fraud-franceAISI-kr-en-toolnames.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Korean <br> *jt3-fraud-franceAISI-kr.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Korean by the Korea AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Swahili <br> *jt3-fraud-franceAISI-kw.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Swahili by the Kenya AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - France AISI - Telugu <br> *jt3-fraud-franceAISI-tl.json* | AgentDojo - evaluation of prompt injection attacks and defenses. Extracted tasks corresponding to fraud attempts in a banking environment. Agentic testing dataset contributed by France AISI, translated into Telugu by the Canada AISI. | MIT License | https://agentdojo.spylab.ai/ |
| Joint Testing 3 - Fraud - HarmBench - Chinese - English tool names <br> *jt3-fraud-hb-ch-en-toolnames.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Chinese <br> *jt3-fraud-hb-ch.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Chinese by the Australia AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - English <br> *jt3-fraud-hb-en.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, in English. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Farsi <br> *jt3-fraud-hb-fa.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Farsi by the Canadian AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - French <br> *jt3-fraud-hb-fr.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into French by the France AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Hindi <br> *jt3-fraud-hb-hn.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Hindi by the Singapore AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Japanese - English tool names <br> *jt3-fraud-hb-ja-en-toolnames.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Japanese <br> *jt3-fraud-hb-ja.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Japanese by the Japan AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Korean - English tool names <br> *jt3-fraud-hb-kr-en-toolnames.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Korean <br> *jt3-fraud-hb-kr.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Korean by the Korea AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Swahili <br> *jt3-fraud-hb-kw.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Swahili by the Kenya AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Fraud - HarmBench - Telugu <br> *jt3-fraud-hb-tl.json* | Agentic testing dataset adapted from HarmBench, concerning fraud risks, translated into Telugu by the Canada AISI. | MIT License | https://github.com/centerforaisafety/HarmBench |
| Joint Testing 3 - Leakage - AgentHarm - Chinese - English tool names <br> *jt3-leakage-ah-ch-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Chinese <br> *jt3-leakage-ah-ch.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Chinese by the Australia AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - English <br> *jt3-leakage-ah-en.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, in English. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Farsi <br> *jt3-leakage-ah-fa.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Farsi by the Canadian AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - French <br> *jt3-leakage-ah-fr.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into French by the France AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Hindi <br> *jt3-leakage-ah-hn.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Hindi by the Singapore AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Japanese - English tool names <br> *jt3-leakage-ah-ja-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Japanese <br> *jt3-leakage-ah-ja.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Japanese by the Japan AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Korean - English tool names <br> *jt3-leakage-ah-kr-en-toolnames.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Korean <br> *jt3-leakage-ah-kr.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Korean by the Korea AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Swahili <br> *jt3-leakage-ah-kw.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Swahili by the Kenya AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - AgentHarm - Telugu <br> *jt3-leakage-ah-tl.json* | Agentic testing dataset adapted from AgentHarm, concerning data leakage risks, translated into Telugu by the Canada AISI. | MIT License | https://huggingface.co/datasets/ai-safety-institute/AgentHarm |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Chinese - English tool names <br> *jt3-leakage-asb-ch-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Chinese <br> *jt3-leakage-asb-ch.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Chinese by the Australia AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - English <br> *jt3-leakage-asb-en.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, in English. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Farsi <br> *jt3-leakage-asb-fa.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Farsi by the Canadian AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - French <br> *jt3-leakage-asb-fr.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into French by the France AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Hindi <br> *jt3-leakage-asb-hn.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Hindi by the Singapore AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Japanese - English tool names <br> *jt3-leakage-asb-ja-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Japanese <br> *jt3-leakage-asb-ja.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Japanese by the Japan AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Korean - English tool names <br> *jt3-leakage-asb-kr-en-toolnames.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Korean <br> *jt3-leakage-asb-kr.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Korean by the Korea AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Swahili <br> *jt3-leakage-asb-kw.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Swahili by the Kenya AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - Agent-SafetyBench - Telugu <br> *jt3-leakage-asb-tl.json* | Agentic testing dataset adapted from Agent-SafetyBench, concerning data leakage risks, translated into Telugu by the Canada AISI. | MIT License | https://github.com/thu-coai/Agent-SafetyBench |
| Joint Testing 3 - Leakage - InjecAgent - Chinese - English tool names <br> *jt3-leakage-ia-ch-en-toolnames.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Chinese by the Australia AISI, with English tool names. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Chinese <br> *jt3-leakage-ia-ch.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Chinese by the Australia AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - English <br> *jt3-leakage-ia-en.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, in English. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Farsi <br> *jt3-leakage-ia-fa.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Farsi by the Canadian AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - French <br> *jt3-leakage-ia-fr.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into French by the France AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Hindi <br> *jt3-leakage-ia-hn.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Hindi by the Singapore AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Japanese - English tool names <br> *jt3-leakage-ia-ja-en-toolnames.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Japanese by the Japan AISI, with English tool names. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Japanese <br> *jt3-leakage-ia-ja.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Japanese by the Japan AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Korean - English tool names <br> *jt3-leakage-ia-kr-en-toolnames.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Korean by the Korea AISI, with English tool names. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Korean <br> *jt3-leakage-ia-kr.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Korean by the Korea AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Swahili <br> *jt3-leakage-ia-kw.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Swahili by the Kenya AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - InjecAgent - Telugu <br> *jt3-leakage-ia-tl.json* | Agentic testing dataset adapted from InjecAgent, concerning data leakage risks, translated into Telugu by the Canada AISI. | MIT License | https://github.com/uiuc-kang-lab/InjecAgent |
| Joint Testing 3 - Leakage - ToolEmu - Chinese - English tool names <br> *jt3-leakage-te-ch-en-toolnames.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Chinese by the Australia AISI, with English tool names. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Chinese <br> *jt3-leakage-te-ch.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Chinese by the Australia AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - English <br> *jt3-leakage-te-en.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, in English. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Farsi <br> *jt3-leakage-te-fa.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Farsi by the Canadian AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - French <br> *jt3-leakage-te-fr.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into French by the France AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Hindi <br> *jt3-leakage-te-hn.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Hindi by the Singapore AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Japanese - English tool names <br> *jt3-leakage-te-ja-en-toolnames.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Japanese by the Japan AISI, with English tool names. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Japanese <br> *jt3-leakage-te-ja.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Japanese by the Japan AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Korean - English tool names <br> *jt3-leakage-te-kr-en-toolnames.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Korean by the Korea AISI, with English tool names. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Korean <br> *jt3-leakage-te-kr.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Korean by the Korea AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Swahili <br> *jt3-leakage-te-kw.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Swahili by the Kenya AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| Joint Testing 3 - Leakage - ToolEmu - Telugu <br> *jt3-leakage-te-tl.json* | Agentic testing dataset adapted from ToolEmu, concerning data leakage risks, translated into Telugu by the Canada AISI. | Apache-2.0 license | https://github.com/ryoungj/ToolEmu |
| m3exam-indonesian <br> *m3exam-indonesian-test.json* | This dataset contains local exam questions for testing Indonesian capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| m3exam-thai <br> *m3exam-thai-test.json* | This dataset contains local exam questions for testing Thai capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| m3exam-vietnamese <br> *m3exam-vietnamese-test.json* | This dataset contains local exam questions for testing Vietnamese capability. | CC BY-NC-SA | https://github.com/DAMO-NLP-SG/M3Exam |
| MedMCQA <br> *medmcqa.json* | MedMCQ is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects. | MIT License | https://github.com/MedMCQA/MedMCQA |
| MedQA (US) <br> *medqa-us.json* | MedQA is a free-form multiple-choice OpenQA dataset for solving medical problems. These are collected from the professional medical board exams. We extracted the list of MCQ questions from the US in this dataset. | MIT License | https://github.com/jind11/MedQA |
| MLC Baseline - Child Abuse and Exploitation <br> *mlc-cae-malicious-user.json* | Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Child Abuse and Exploitation <br> *mlc-cae-typical-user.json* | Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives <br> *mlc-cbr-malicious-user.json* | Responses that enable, encourage, or endorse the creation of weapons of mass destruction. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives <br> *mlc-cbr-typical-user.json* | Responses that enable, encourage, or endorse the creation of weapons of mass destruction. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Hate <br> *mlc-hat-malicious-user.json* | Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Hate <br> *mlc-hat-typical-user.json* | Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| ML Commons AILuminate - Intellectual Property Violation - Cantonese <br> *mlc-ipv-ca.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Chinese <br> *mlc-ipv-cn.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - English <br> *mlc-ipv-en.json* | Test whether responses violate the intellectual property rights of any party. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Farsi <br> *mlc-ipv-fa.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - French <br> *mlc-ipv-fr.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Japanese <br> *mlc-ipv-jp.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Korean <br> *mlc-ipv-kr.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Swahili <br> *mlc-ipv-kw.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Malay <br> *mlc-ipv-my.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Intellectual Property Violation - Telugu <br> *mlc-ipv-tl.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Chinese <br> *mlc-ncr-cn.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - English <br> *mlc-ncr-en.json* | Test whether responses enable, encourage, or endorse the commission of non-violent crimes. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Farsi <br> *mlc-ncr-fa.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - French <br> *mlc-ncr-fr.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Japanese <br> *mlc-ncr-jp.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Korean <br> *mlc-ncr-kr.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Swahili <br> *mlc-ncr-kw.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Malay <br> *mlc-ncr-my.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Non-Violent Crimes - Telugu <br> *mlc-ncr-tl.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| MLC Baseline - Non-Violent Crimes <br> *mlc-nvc-malicious-user.json* | Responses that enable, encourage, or endorse the commission of non-violent crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Non-Violent Crimes <br> *mlc-nvc-typical-user.json* | Responses that enable, encourage, or endorse the commission of non-violent crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| ML Commons AILuminate - Privacy - Chinese <br> *mlc-prv-cn.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons  AILuminate - Privacy - English <br> *mlc-prv-en.json* | Test whether responses contain sensitive, nonpublic personal information that could undermine someoneâ€™s physical, digital, or financial security. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Farsi <br> *mlc-prv-fr.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Japanese <br> *mlc-prv-jp.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Korean <br> *mlc-prv-kr.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Swahili <br> *mlc-prv-kw.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Malay <br> *mlc-prv-my.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Privacy - Telugu <br> *mlc-prv-tl.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| MLC Baseline - Sex-Related Crimes <br> *mlc-scr-malicious-user.json* | Responses that enable, encourage, or endorse the commission of sex-related crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Sex-Related Crimes <br> *mlc-scr-typical-user.json* | Responses that enable, encourage, or endorse the commission of sex-related crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Suicide & Self-Harm <br> *mlc-ssh-typical-user.json* | Responses that enable, encourage, or endorse acts of intentional self-harm. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MLC Baseline - Suicide & Self-Harm <br> *mlc-ssh-vulnerable-user.json* | Responses that enable, encourage, or endorse acts of intentional self-harm. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| ML Commons AILuminate - Violent Crimes - Chinese <br> *mlc-vcr-cn.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - English <br> *mlc-vcr-en.json* | Test whether responses enable, encourage, or endorse the commission of violent crimes. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - French <br> *mlc-vcr-fr.json* | A dataset of security-related questions and answers. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - Japanese <br> *mlc-vcr-jp.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - Korean <br> *mlc-vcr-kr.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - Swahili <br> *mlc-vcr-kw.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| MLC Baseline - Violent Crimes <br> *mlc-vcr-malicious-user.json* | Responses that enable, encourage, or endorse the commission of violent crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| ML Commons AILuminate - Violent Crimes - Malay <br> *mlc-vcr-my.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| ML Commons AILuminate - Violent Crimes - Telugu <br> *mlc-vcr-tl.json* | This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well. | CC-BY-4.0 license | https://mlcommons.org/ailuminate/ |
| MLC Baseline - Violent Crimes <br> *mlc-vcr-typical-user.json* | Responses that enable, encourage, or endorse the commission of violent crimes. | CC-BY-4.0 license | https://arxiv.org/html/2404.12241v1 |
| MMLU <br> *mmlu-all.json* | This dataset covers 57 tasks including elementary mathemathics, US history, computer science, law, and more. | MIT license | https://github.com/hendrycks/test |
| MMLU Anatomy <br> *mmlu-anatomy.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Anatomy | MIT License | https://github.com/hendrycks/test |
| MMLU Clinical Knowledge <br> *mmlu-clinicalknowledge.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Clinical Knowledge | MIT License | https://github.com/hendrycks/test |
| MMLU College Biology <br> *mmlu-collegebiology.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Biology | MIT License | https://github.com/hendrycks/test |
| MMLU College Medicine <br> *mmlu-collegemedicine.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Medicine | MIT License | https://github.com/hendrycks/test |
| MMLU Medical Genetics <br> *mmlu-medicalgenetics.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Medical Genetics | MIT License | https://github.com/hendrycks/test |
| MMLU Professional Medicine.json <br> *mmlu-professionalmedicine.json* | MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Professional Medicine.json | MIT License | https://github.com/hendrycks/test |
| List of Occupations <br> *occupation.json* | This dataset contains a list of occupations collected from various websites (e.g., MoM of Singapore). The dataset can be used to test job-related applications. | MIT License | IMDA |
| enronemail <br> *privacy-enronemails.json* | The Enron email dataset is aimed at evaluating if there is leakage of private information. The dataset was used in DecodingTrust as part of its privacy evaluation. | Apache-2.0 | https://github.com/ParakweetLabs/EmailIntentDataSet |
| Jailbreak Prompts <br> *prompt_injection_jailbreak.json* | This dataset evaluates whether a model/app will detect attempted jailbreaks and generate unsafe content. These jailbreak techniques are collected from X (e.g., Pliny) and public repository (e.g., in-the-wild jailbreak prompts). | Apache 2.0 | OWASP |
| PubMedQA <br> *pubmedqa.json* | PubMedQA is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects. | MIT License | https://github.com/MedMCQA/MedMCQA |
| mock-dataset <br> *rag-sample-dataset.json* | This is a sample of a dataset to be used for RAG Evaluation. |  |  |
| RealToxicityPrompts <br> *real-toxicity-prompts.json* | RealToxicityPrompts aims to measure the model's ability in identify toxic sentences. | Apache-2.0 | https://github.com/allenai/real-toxicity-prompts/ |
| RealtimeQA <br> *realtimeqa-past.json* | RealTime QA is a dynamic question answering (QA) platform that inquires about the present. |  | https://github.com/realtimeqa/realtimeqa_public |
| sg-legal-glossary <br> *sg-legal-glossary.json* | A list of singapore legal terms extracted from SICC and Judiciary websites. |  | https://www.sicc.gov.sg/glossary-of-legal-terms |
| sg-university-tutorial-questions-legal <br> *sg-university-tutorial-questions-legal.json* | Contain tutorial questions and answers from Singapore's Universities to test model's ability in understanding legal context in Singapore |  |  |
| Facts about Singapore in True and False <br> *singapore-facts-tnf.json* | Contain prompts that contains facts about Singapore, in True/False format | Apache-2.0 | IMDA |
| Food in Singapore <br> *singapore-food-tnf.json* | Contain prompts that test model's understanding in Food, in True/False format | Apache-2.0 | IMDA |
| Iconic Places in Singapore <br> *singapore-iconic-places.json* | Contain questions about Singapore's iconic places. | Apache-2.0 | IMDA |
| Places in Singapore <br> *singapore-places-tnf.json* | Contain prompts that test model's understanding places in Singapore, in True/False format | Apache-2.0 | IMDA |
| Singapore Political History <br> *singapore-political-history.json* | Contain questions about Singapore's key historical events in political. | Apache-2.0 | IMDA |
| Singapore Public Housing <br> *singapore-public-housing.json* | Contain questions about Singapore's public housing system | Apache-2.0 | IMDA |
| Safety Benchmark (Singapore Context) <br> *singapore-safety-questions.json* | Contain prompts that test safety in Singapore-context | Apache-2.0 | IMDA |
| Singapore Transport System <br> *singapore-transport-system.json* | Contain questions about Singapore's transport system | Apache-2.0 | IMDA |
| squad-shifts-tnf <br> *squad-shifts-tnf.json* | Zero-shot reading comprehension on paragraphs and questions from squadshifts | Apache 2.0 | https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/squad_shifts |
| squad-v2 <br> *squad-v2.json* | Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable. | CC BY-SA 4.0 | rajpurkar/squad_v2 Â· Datasets at Hugging Face |
| tamil-thirukural <br> *tamil-kural-classification.json* | This dataset is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets. | Creative Commons Attribution 4.0 International | https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset |
| tamil-news-classification <br> *tamil-tamilnews-classification.json* | This dataset is used to classify a static set of Tamil News. The task is to classify news to its respective category. The dataset has 6 news categories - "tamilnadu", "india", "cinema", "sports", "politics", "world". | GNU General Public License v3.0 | https://github.com/vanangamudi/tamil-news-classification/tree/master/dataset/news |
| tanglish-tweets-SA <br> *tamil-tanglish-tweets.json* | Code-mixed Tamil and English tweets curated for the sentiment analysis task. | CC0: Public Domain | https://www.kaggle.com/datasets/vyombhatia/tanglish-comments-for-sentiment-ananlysis/data |
| TruthfulQA (MCQ Version) <br> *truthfulqa-mcq.json* | TruthfulQA aims to measure the the truthfulness of a model. This dataset uses a multiple choice format. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| truthfulqa <br> *truthfulqa-multiple-open-ended.json* | TruthfulQA aims to measure the the truthfulness of a model. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| truthfulqa <br> *truthfulqa-open-ended.json* | TruthfulQA aims to measure the the truthfulness of a model. | Apache-2.0 | https://github.com/sylinrl/TruthfulQA |
| uciadult <br> *uciadult.json* | The UCI adult dataset, created in 1996, is used to train models to predict whether a person's income will exceed $50K/yr based on census data. Also known as "Census Income" dataset. | Creative Commons Attribution 4.0 International | https://archive.ics.uci.edu/dataset/2/adult |
| winobias-variation1 <br> *winobias-type1.json* | This dataset contains gender-bias based on the professions from the Labor Force Statistics (https://www.bls.gov/cps/cpsaat11.htm), which contain some gender-bias. | MIT License | https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino |
| Winogrande <br> *winogrande.json* | This dataset is used for commonsense reasoning, expert-crafted pronoun resolution problems designed to be unsolvable for statistical models. | Apache-2.0 | https://github.com/allenai/winogrande |