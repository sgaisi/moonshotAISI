# Product Context

## Why This Project Exists
Moonshot AI provides comprehensive testing and evaluation capabilities for Large Language Models (LLMs), helping organizations ensure their AI systems are safe, reliable, and perform as expected before deployment.

## Problems It Solves
1. **LLM Safety Assessment** - Identifies potential risks and vulnerabilities in AI models
2. **Performance Benchmarking** - Measures model capabilities against standardized tests
3. **Agentic AI Evaluation** - Tests AI agents' ability to use tools and perform complex tasks
4. **Regulatory Compliance** - Helps meet AI safety and governance requirements

## Current Gap: Agentic Testing UI
While benchmarking and red teaming have full UI support, agentic testing capabilities are only available through CLI. This creates:
- **Usability barriers** for non-technical users
- **Workflow inconsistency** across testing modes  
- **Limited adoption** of agentic testing features
- **Integration challenges** for teams using the web interface

## Target Users
- **AI Safety Engineers** - Need comprehensive testing tools
- **Model Developers** - Require performance validation
- **Compliance Teams** - Must demonstrate AI system safety
- **Research Teams** - Need standardized evaluation frameworks

## User Experience Goals
1. **Consistency** - Agentic testing should feel identical to benchmarking
2. **Accessibility** - Non-CLI users can perform agentic tests
3. **Efficiency** - Streamlined workflow from setup to results
4. **Flexibility** - Support for various cookbooks and model configurations
5. **Transparency** - Clear visibility into test progress and results

## Success Metrics
- Feature parity between agentic and benchmarking workflows
- User adoption of agentic testing through UI
- Reduced time-to-test for agentic evaluations
- Positive user feedback on workflow consistency
